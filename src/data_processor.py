import json
import re
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass, asdict


@dataclass
class MedicalDocument:
    """Cáº¥u trÃºc dá»¯ liá»‡u y khoa Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½"""
    id: str
    title: str
    url: str
    content: str
    authors: List[str]
    reviewers: List[str]
    references: List[str]
    sections: Dict[str, str]
    metadata: Dict[str, Any]


class MedicalDataProcessor:
    """Xá»­ lÃ½ dá»¯ liá»‡u y khoa tá»« MSD Manuals Ä‘á»ƒ training AI"""

    def __init__(self, data_dir: str = "data_msd"):
        self.data_dir = Path(data_dir)

    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch text, loáº¡i bá» kÃ½ tá»± thá»«a vÃ  lá»—i encoding"""
        if not text:
            return ""

        # Fix cÃ¡c lá»—i encoding phá»• biáº¿n - PHáº¢I lÃ m TRÆ¯á»šC khi xá»­ lÃ½ space
        text = text.replace("hÃ nh v i", "hÃ nh vi")
        text = text.replace("v i ", "vi ")
        text = text.replace("á»›n láº¡nh", "run láº¡nh")
        text = text.replace("chá»c dá»", "chá»c dÃ²")
        text = text.replace("thÃ i ra", "tháº£i ra")

        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t
        text = text.replace('|', ' ')

        # Loáº¡i bá» nhiá»u space liÃªn tiáº¿p (PHáº¢I sau khi fix encoding)
        text = re.sub(r'\s+', ' ', text)

        # Loáº¡i bá» space trÆ°á»›c dáº¥u cÃ¢u
        text = re.sub(r'\s+([.,;:!?])', r'\1', text)

        return text.strip()

    def extract_metadata(self, content: str) -> Dict[str, Any]:
        """TrÃ­ch xuáº¥t metadata tá»« ná»™i dung"""
        lines = content.split('\n')

        title = ""
        url = ""
        for line in lines[:5]:
            if line.startswith("TIÃŠU Äá»€:"):
                title = line.replace("TIÃŠU Äá»€:", "").strip()
            elif line.startswith("URL:"):
                url = line.replace("URL:", "").strip()

        return {
            'title': title,
            'url': url,
            'source': 'MSD Manuals',
            'language': 'vi'
        }

    def extract_authors_improved(self, content: str) -> tuple[List[str], List[str]]:
        """TrÃ­ch xuáº¥t tÃ¡c giáº£ vÃ  ngÆ°á»i xem xÃ©t"""
        authors = []
        reviewers = []

        # TÃ¬m pháº§n tÃ¡c giáº£ (tá»« "Theo" Ä‘áº¿n "Xem xÃ©t bá»Ÿi")
        author_pattern = r'Theo\s+(.*?)(?=Xem xÃ©t bá»Ÿi|ÄÃ£ xem xÃ©t)'
        author_match = re.search(author_pattern, content, re.DOTALL)

        if author_match:
            author_text = author_match.group(1)
            # Láº¥y táº¥t cáº£ dÃ²ng cÃ³ ná»™i dung
            lines = [line.strip() for line in author_text.split('\n') if line.strip()]

            # TÃ¬m tÃªn (dÃ²ng khÃ´ng pháº£i dáº¥u pháº©y Ä‘Æ¡n, khÃ´ng pháº£i MD/PhD, khÃ´ng pháº£i University...)
            for line in lines:
                # Bá» qua dÃ²ng chá»‰ cÃ³ dáº¥u pháº©y hoáº·c chá»©c danh
                if line in [',', 'MD', 'PhD', 'MPH', 'MACP']:
                    continue
                # Bá» qua dÃ²ng cÃ³ tÃªn tá»• chá»©c
                if any(x in line for x in ['University', 'College', 'School', 'Hospital', 'Center']):
                    continue
                # Náº¿u tÃ¬m tháº¥y tÃªn há»£p lá»‡
                if 5 < len(line) < 50 and any(c.isalpha() for c in line):
                    # Loáº¡i bá» dáº¥u pháº©y cuá»‘i náº¿u cÃ³
                    name = line.rstrip(',').strip()
                    authors.append(name)
                    break

        # TÃ¬m ngÆ°á»i xem xÃ©t
        reviewer_pattern = r'Xem xÃ©t bá»Ÿi\s+(.*?)(?=ÄÃ£ xem xÃ©t)'
        reviewer_match = re.search(reviewer_pattern, content, re.DOTALL)

        if reviewer_match:
            reviewer_text = reviewer_match.group(1)
            lines = [line.strip() for line in reviewer_text.split('\n') if line.strip()]

            for line in lines:
                if line in [',', 'MD', 'PhD', 'MPH', 'MACP']:
                    continue
                if any(x in line for x in ['University', 'College', 'School', 'Hospital', 'Center', 'Division']):
                    continue
                if 5 < len(line) < 50 and any(c.isalpha() for c in line):
                    name = line.rstrip(',').strip()
                    reviewers.append(name)
                    break

        return authors, reviewers

    def extract_main_content(self, content: str) -> str:
        """TrÃ­ch xuáº¥t ná»™i dung chÃ­nh, loáº¡i bá» header vÃ  footer"""
        # TÃ¬m pháº§n ná»™i dung chÃ­nh
        main_start = content.find("Ná»˜I DUNG:")
        if main_start == -1:
            return content

        main_content = content[main_start + len("Ná»˜I DUNG:"):]

        # BÆ°á»›c 1: TÃ¬m vá»‹ trÃ­ Káº¾T THÃšC metadata (sau "ÄÃ£ xem xÃ©t/ÄÃ£ chá»‰nh sá»­a")
        # ThÆ°á»ng cÃ³ pattern: "ÄÃ£ xem xÃ©t/ÄÃ£ chá»‰nh sá»­a [newline] Ä‘Ã£ sá»­a Ä‘á»•i [newline] Thg X 20XX [newline] vXXXXX_vi"
        metadata_end_pattern = r'ÄÃ£ xem xÃ©t/ÄÃ£ chá»‰nh sá»­a.*?v\d+_vi'
        metadata_match = re.search(metadata_end_pattern, main_content, re.DOTALL)

        if metadata_match:
            # Cáº¯t tá»« sau metadata
            main_content = main_content[metadata_match.end():]
        else:
            # Fallback: bá» qua cÃ¡c dÃ²ng metadata thá»§ cÃ´ng
            lines = main_content.split('\n')
            content_lines = []
            skip_metadata = True

            for line in lines:
                line_stripped = line.strip()

                if not line_stripped:
                    continue

                # Kiá»ƒm tra náº¿u lÃ  metadata
                is_metadata = any(x in line_stripped for x in [
                    'Theo', 'Xem xÃ©t bá»Ÿi', 'ÄÃ£ xem xÃ©t', 'ÄÃ£ chá»‰nh sá»­a',
                    'MD,', 'PhD,', 'University', 'College', 'Hospital',
                    'v_vi', 'Thg', 'Ä‘Ã£ sá»­a Ä‘á»•i'
                ])

                # Náº¿u khÃ´ng pháº£i metadata
                if not is_metadata:
                    skip_metadata = False

                if not skip_metadata:
                    content_lines.append(line_stripped)

            main_content = ' '.join(content_lines)

        # Loáº¡i bá» footer
        footer_patterns = [
            "Test your Knowledge",
            "Take a Quiz!",
            "Báº£n quyá»n",
            "Â© 2025",
            "Â© 2024",
            "Â© 2023",
            "Merck & Co."
        ]

        for pattern in footer_patterns:
            idx = main_content.find(pattern)
            if idx != -1:
                main_content = main_content[:idx]
                break

        # LÃ m sáº¡ch
        main_content = self.clean_text(main_content)

        return main_content

    def extract_references(self, content: str) -> List[str]:
        """TrÃ­ch xuáº¥t cÃ¡c tham chiáº¿u (Xem thÃªm...)"""
        references = []

        # Pattern: (Xem thÃªm ...)
        pattern1 = r'\(Xem thÃªm\s+([^)]+)\)'
        matches = re.findall(pattern1, content)
        for match in matches:
            # TÃ¡ch náº¿u cÃ³ "vÃ  xem"
            if 'vÃ  xem' in match or 'vÃ ' in match:
                parts = re.split(r'\s+vÃ \s+(?:xem\s+)?', match)
                for part in parts:
                    part = part.strip('. ')
                    # Chá»‰ láº¥y náº¿u Ä‘á»™ dÃ i há»£p lÃ½ vÃ  khÃ´ng chá»©a metadata
                    if 20 < len(part) < 150 and not any(x in part for x in ['ÄÃ£ xem xÃ©t', 'Ä‘Ã£ sá»­a Ä‘á»•i', 'Thg', '_vi']):
                        references.append(self.clean_text(part))
            else:
                match = match.strip('. ')
                if 20 < len(match) < 150 and not any(x in match for x in ['ÄÃ£ xem xÃ©t', 'Ä‘Ã£ sá»­a Ä‘á»•i', 'Thg', '_vi']):
                    references.append(self.clean_text(match))

        # Loáº¡i bá» trÃ¹ng láº·p
        references = list(dict.fromkeys(references))

        return references[:10]

    def extract_sections(self, content: str) -> Dict[str, str]:
        """TrÃ­ch xuáº¥t cÃ¡c pháº§n cá»§a tÃ i liá»‡u"""
        sections = {}

        # CÃ¡c tiÃªu Ä‘á» pháº§n thÆ°á»ng gáº·p
        section_headers = [
            "CÄƒn nguyÃªn",
            "Sinh lÃ½ bá»‡nh",
            "Triá»‡u chá»©ng vÃ  Dáº¥u hiá»‡u",
            "Triá»‡u chá»©ng",
            "Cháº©n Ä‘oÃ¡n",
            "Äiá»u trá»‹",
            "PhÃ²ng ngá»«a",
            "Nhá»¯ng Ä‘iá»ƒm chÃ­nh",
            "TiÃªn lÆ°á»£ng",
            "PhÃ¢n loáº¡i",
            "CÃ¡c biáº¿n chá»©ng"
        ]

        # TÃ¬m táº¥t cáº£ vá»‹ trÃ­ cá»§a headers
        header_positions = []
        for header in section_headers:
            # Pattern 1: "Header |" hoáº·c "| Header" (cÃ³ thá»ƒ cÃ³ space xung quanh)
            pattern1 = rf'\|\s*{re.escape(header)}\s*\|'
            for match in re.finditer(pattern1, content):
                header_positions.append((match.end(), header))

            # Pattern 2: Header Ä‘á»©ng riÃªng, theo sau lÃ  ná»™i dung (khÃ´ng pháº£i header khÃ¡c ngay sau)
            # TÃ¬m: "Header" + space/newline + text (khÃ´ng pháº£i header khÃ¡c)
            pattern2 = rf'{re.escape(header)}\s+([A-ZÃ€Ãáº¢Ãƒáº Ä‚áº°áº®áº²áº´áº¶Ã‚áº¦áº¤áº¨áºªáº¬ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÃŒÃá»ˆÄ¨á»Š])'
            for match in re.finditer(pattern2, content):
                # Kiá»ƒm tra xem text sau header cÃ³ pháº£i header khÃ¡c khÃ´ng
                text_after = content[match.start(1):match.start(1) + 50]
                is_another_header = any(h in text_after for h in section_headers if h != header)
                if not is_another_header:
                    header_positions.append((match.start(1), header))

        # Sáº¯p xáº¿p theo vá»‹ trÃ­
        header_positions.sort()

        # TrÃ­ch xuáº¥t ná»™i dung giá»¯a cÃ¡c headers
        for i, (start_pos, header) in enumerate(header_positions):
            # TÃ¬m vá»‹ trÃ­ káº¿t thÃºc
            if i < len(header_positions) - 1:
                content_end = header_positions[i + 1][0]
            else:
                content_end = len(content)

            # Láº¥y ná»™i dung
            section_content = content[start_pos:content_end].strip()

            # Loáº¡i bá» cÃ¡c kÃ½ tá»± phÃ¢n cÃ¡ch thá»«a á»Ÿ Ä‘áº§u
            section_content = section_content.lstrip('| \n\t')

            # Chá»‰ lÆ°u náº¿u cÃ³ ná»™i dung há»£p lÃ½
            if section_content and 50 < len(section_content) < 20000:
                sections[header] = self.clean_text(section_content)

        return sections

    def process_file(self, filepath: Path) -> MedicalDocument:
        """Xá»­ lÃ½ má»™t file"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        # TrÃ­ch xuáº¥t cÃ¡c thÃ nh pháº§n
        metadata = self.extract_metadata(content)
        authors, reviewers = self.extract_authors_improved(content)
        main_content = self.extract_main_content(content)
        references = self.extract_references(content)
        sections = self.extract_sections(main_content)

        # Táº¡o ID tá»« filename
        doc_id = filepath.stem.lower().replace(' ', '_').replace(',', '')

        return MedicalDocument(
            id=doc_id,
            title=metadata['title'],
            url=metadata['url'],
            content=main_content,
            authors=authors,
            reviewers=reviewers,
            references=references,
            sections=sections,
            metadata=metadata
        )

    def process_all(self) -> List[MedicalDocument]:
        """Xá»­ lÃ½ táº¥t cáº£ file trong thÆ° má»¥c"""
        documents = []

        txt_files = sorted(self.data_dir.glob("*.txt"))
        print(f"TÃ¬m tháº¥y {len(txt_files)} file")

        for i, filepath in enumerate(txt_files, 1):
            try:
                doc = self.process_file(filepath)
                documents.append(doc)
                print(f"[{i}/{len(txt_files)}] âœ“ {filepath.name}")
            except Exception as e:
                print(f"[{i}/{len(txt_files)}] âœ— {filepath.name}: {e}")

        return documents

    def save_to_json(self, documents: List[MedicalDocument], output_file: str = "processed_medical_data.json"):
        """LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ ra JSON"""
        data = [asdict(doc) for doc in documents]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ ÄÃ£ lÆ°u {len(documents)} documents vÃ o {output_file}")
        return output_file

    def save_to_jsonl(self, documents: List[MedicalDocument], output_file: str = "processed_medical_data.jsonl"):
        """LÆ°u ra JSONL"""
        with open(output_file, 'w', encoding='utf-8') as f:
            for doc in documents:
                json.dump(asdict(doc), f, ensure_ascii=False)
                f.write('\n')

        print(f"âœ“ ÄÃ£ lÆ°u {len(documents)} documents vÃ o {output_file}")
        return output_file

    def create_training_dataset(self, documents: List[MedicalDocument], output_file: str = "training_data.jsonl"):
        """Táº¡o dataset training"""
        training_data = []

        for doc in documents:
            # Format 1: Question-Answer
            training_data.append({
                "instruction": f"{doc.title} lÃ  gÃ¬?",
                "input": "",
                "output": doc.content[:500] + "..." if len(doc.content) > 500 else doc.content,
                "metadata": {
                    "source": doc.url,
                    "title": doc.title
                }
            })

            # Format 2: Section-based
            for section_name, section_content in doc.sections.items():
                if len(section_content) > 50:
                    training_data.append({
                        "instruction": f"Giáº£i thÃ­ch vá» {section_name.lower()} cá»§a {doc.title}",
                        "input": "",
                        "output": section_content,
                        "metadata": {
                            "source": doc.url,
                            "title": doc.title,
                            "section": section_name
                        }
                    })

        with open(output_file, 'w', encoding='utf-8') as f:
            for item in training_data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')

        print(f"âœ“ ÄÃ£ táº¡o {len(training_data)} training examples vÃ o {output_file}")
        return output_file

    def generate_statistics(self, documents: List[MedicalDocument]):
        """Táº¡o thá»‘ng kÃª"""
        total_chars = sum(len(doc.content) for doc in documents)
        total_sections = sum(len(doc.sections) for doc in documents)
        total_refs = sum(len(doc.references) for doc in documents)

        print("\n" + "=" * 60)
        print("THá»NG KÃŠ Dá»® LIá»†U:")
        print("=" * 60)
        print(f"ğŸ“„ Tá»•ng sá»‘ documents: {len(documents)}")
        print(f"ğŸ“ Tá»•ng sá»‘ kÃ½ tá»±: {total_chars:,}")
        print(f"ğŸ“Š Trung bÃ¬nh kÃ½ tá»±/doc: {total_chars // len(documents) if documents else 0:,}")
        print(f"ğŸ”– Tá»•ng sá»‘ sections: {total_sections}")
        print(f"ğŸ”— Tá»•ng sá»‘ references: {total_refs}")
        print(f"ğŸ‘¥ Documents cÃ³ tÃ¡c giáº£: {sum(1 for doc in documents if doc.authors)}")
        print(f"âœ… Documents cÃ³ reviewer: {sum(1 for doc in documents if doc.reviewers)}")
        print("=" * 60)


def main():
    """Main function"""
    processor = MedicalDataProcessor("../data_msd")

    print("ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ dá»¯ liá»‡u y khoa...")
    print("=" * 60)

    # Xá»­ lÃ½ táº¥t cáº£ file
    documents = processor.process_all()

    if not documents:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y document nÃ o!")
        return

    print("\nğŸ’¾ Äang lÆ°u dá»¯ liá»‡u...")

    # LÆ°u ra nhiá»u format
    processor.save_to_json(documents, "medical_data.json")
    processor.save_to_jsonl(documents, "medical_data.jsonl")
    processor.create_training_dataset(documents, "training_data.jsonl")

    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    processor.generate_statistics(documents)

    # Hiá»ƒn thá»‹ vÃ­ dá»¥
    if documents:
        print("\n" + "=" * 60)
        print("VÃ Dá»¤ DOCUMENT Äáº¦U TIÃŠN:")
        print("=" * 60)
        doc = documents[0]
        print(f"ğŸ“Œ ID: {doc.id}")
        print(f"ğŸ“„ Title: {doc.title}")
        print(f"ğŸ‘¤ Authors: {', '.join(doc.authors) if doc.authors else 'N/A'}")
        print(f"ğŸ‘¥ Reviewers: {', '.join(doc.reviewers) if doc.reviewers else 'N/A'}")
        print(f"ğŸ”— References: {len(doc.references)}")
        print(f"ğŸ“‘ Sections: {', '.join(doc.sections.keys())}")
        print(f"ğŸ“ Content preview: {doc.content[:200]}...")
        print("=" * 60)


if __name__ == "__main__":
    main()